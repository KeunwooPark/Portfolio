<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href=/assets/css/header_footer.css>
		<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>
		<link rel="stylesheet" href=/assets/bootstrap-4.3.1-dist/css/bootstrap.min.css>
		<script scr=/assets/bootstrap-4.3.1-dist/js/bootstrap.min.js></script>
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-YG4YGWN03V"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'G-YG4YGWN03V');
		</script>
		
  </head>
  <body>
		<div class="layout-content">
			<title> OddEyeCam: A Sensing Technique for Body-Centric Peephole Interaction using WFoV RGB and NFoV Depth Cameras </title>
<link rel=stylesheet type=text/css href=/assets/css/article.css>
<div class='container'>
  <div class='main'>
    <h1 class="text-center mt-5 mb-3"> OddEyeCam: A Sensing Technique for Body-Centric Peephole Interaction using WFoV RGB and NFoV Depth Cameras </h1>
    <h3 class="text-center"> <p>Daehwa Kim, <strong>Keunwoo Park</strong>, Geehyuk Lee</p>
 </h3>
    <h3 class="text-center text-muted"> UIST 2020 </h3>
    
    <div class="media_div mb-5">
      
      <div class="video_div embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" width="560" height="315" src = "https://www.youtube.com/embed/R56iuEuZyo0" frameborder="0" allowfullscreen></iframe></div>
      
    </div>
    
    <div class="content"><h1 id="abstract">Abstract</h1>
<p>The space around the body not only expands the interaction space of a mobile device beyond its small screen, but also enables users to utilize their kinesthetic sense. Therefore, body-centric peephole interaction has gained considerable attention. To support its practical implementation, we propose OddEyeCam, which is a vision-based method that tracks the 3D location of a mobile device in an absolute, wide, and continuous manner with respect to the body of a user in both static and mobile environments. OddEyeCam tracks the body of a user using a wide-view RGB camera and obtains precise depth information using a narrow-view depth camera from a smartphone close to the body. We quantitatively evaluated OddEyeCam through an accuracy test and two user studies. The accuracy test showed the average tracking accuracy of OddEyeCam was 4.17 and 4.47cm in 3D space when a participant is standing and walking, respectively. In the first user study, we implemented various interaction scenarios and observed that OddEyeCam was well received by the participants. In the second user study, we observed that the peephole target acquisition task performed using our system followed Fitts’ law. We also analyzed the performance of OddEyeCam using the obtained measurements and observed that the participants completed the tasks with sufficient speed and accuracy.</p>

<h1 id="technical-concept">Technical Concept</h1>
<p>OddEyeCam estimates the 3D location of a smartphone with respect to the body The process is</p>

<ol>
  <li>
    <p>A WFoV (Wide Field of View) RGB camera provides a whole-body image.</p>
  </li>
  <li>
    <p>An RGB-D camera provides partial body depth and RGB image.</p>
  </li>
  <li>
    <p>An distortion-alleviation module reduces the distortion of the fisheye image through an equirectangular projection. A
body-tracking algorithm provides body keypoints from the undistorted image.</p>
  </li>
  <li>
    <p>The body keypoints found in the WFoV image can be projected to the NFoV (Near Field of View) depth image by combining two cameras.</p>
  </li>
  <li>
    <p>Using keypoints and depth information, as well as additional gravity vector from an accelerometer, we can estimate the body coordinate system.</p>
  </li>
  <li>
    <p>We can obtain the device location with respect to the body by converting the body position (obtained in Step 5) with respect to the camera.</p>
  </li>
</ol>

<p><img src="/assets/img/oddeyecam/pipeline.png" alt="pipeline" /></p>

<h1 id="hardware-prototype">Hardware Prototype</h1>
<p>We used a 180° fsheyeeye lens USB camera as the wide field of view RGB camera. Intel RealSense D415 was chosen as the narrow field of view depth camera in our prototype.</p>

<p><img src="/assets/img/oddeyecam/hardware.png" alt="hardware" /></p>

<h1 id="example-applications">Example Applications</h1>
<p>We created several applications to show various design possibilities of OddEyeCam.
<img src="/assets/img/oddeyecam/applications.png" alt="applications" /></p>

<ul>
  <li>A. Drag and Drop Between Apps</li>
  <li>B. Body-Centric Folder</li>
  <li>C. Large-Image Viewer</li>
  <li>D. One-Hand Tagging</li>
  <li>E. Getting Directions</li>
  <li>F. Marking Menu</li>
</ul>

<p><strong>For more detail, please see our paper.</strong></p>

<h1 id="resources">Resources</h1>

<ul>
  <li><a href="https://dl.acm.org/doi/10.1145/3379337.3415889">paper</a></li>
  <li><a href="https://youtu.be/rsiCohRoFYI">short talk</a></li>
  <li><a href="https://youtu.be/u6xvk0dM8fo">long talk</a></li>
  <li><a href="https://github.com/KAIST-HCIL/OddEyeCam">code</a></li>
</ul>

<h1 id="my-contributions">My Contributions</h1>

<p>I participated in</p>
<ul>
  <li>project ideation</li>
  <li>sensing algorithm design</li>
  <li>user study design and its software implementation</li>
</ul>
</div>
    
  </div>
</div>

		</div>
	</body>
  <footer class="mb-5">
    <a href="/">back to home</a>
  </footer>
</html>
